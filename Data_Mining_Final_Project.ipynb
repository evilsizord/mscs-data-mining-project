{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM47AKNY12zEcrOcORVKKe7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evilsizord/mscs-data-mining-project/blob/main/Data_Mining_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Review Classification\n",
        "Final Project \\\n",
        "CSE 5334 - Data Mining \\\n",
        "Daniel Evilsizor \\\n",
        "November 13, 2022"
      ],
      "metadata": {
        "id": "XQM0pRlgfLq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data from Kaggle.com\n",
        "\n",
        "# NOTE: Requires you to have a Kaggle.com account. From your account you can generate an API key.\n",
        "# It will be provided in kaggle.json. Upload the JSON file to this project BEFORE RUNNING.\n",
        "\n",
        "# src: https://www.kaggle.com/general/74235\n",
        "\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download vivekgediya/ecommerce-product-review-data\n",
        "! mkdir dataset\n",
        "! unzip ecommerce-product-review-data.zip -d dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhmeJ5GQfVE3",
        "outputId": "ffc6d10c-3a02-462c-d0b2-a97854555615"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ecommerce-product-review-data.zip to /content\n",
            " 82% 7.00M/8.55M [00:00<00:00, 13.4MB/s]\n",
            "100% 8.55M/8.55M [00:00<00:00, 10.9MB/s]\n",
            "Archive:  ecommerce-product-review-data.zip\n",
            "  inflating: dataset/Automotive_5.json  \n",
            "  inflating: dataset/Flipkart_Reviews - Electronics.csv  \n",
            "  inflating: dataset/Product Review Data.csv  \n",
            "  inflating: dataset/Product Review Large Data.csv  \n",
            "  inflating: dataset/electronics_reviews_uniq.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load all necessary libraries\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import pandas\n",
        "import nltk\n",
        "#from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')  # needed for tokenizer\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia4MspXxfWl7",
        "outputId": "2065ae29-a955-41cc-ceb3-c912c0c1cdb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTokenizer:\n",
        "  def __init__(self):\n",
        "    self.stemmer = SnowballStemmer(\"english\")\n",
        "    self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  def tokenize(self, doc):\n",
        "    # strip punctuation, it's not needed\n",
        "    # ref: https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string\n",
        "    chars = \"/*[]()>#+,.!;:?\"\n",
        "    for c in chars:\n",
        "        doc = doc.replace(c, \" \")\n",
        "    tokens = word_tokenize(doc)\n",
        "    # remove stop words\n",
        "    # ref: https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
        "    # Also, convert to a set so we only get unique words per row (no duplicates)\n",
        "    words = set()\n",
        "    for tok in tokens:\n",
        "      word = self.stemmer.stem(tok)\n",
        "      if not word.lower() in self.stop_words:\n",
        "        words.add(word)\n",
        "    return words\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "\n",
        "  def __init__(self, vocab, classes, tokenizer):\n",
        "    self.class_counts = self.new_default_zero_dict()\n",
        "    self.class_probs = self.new_default_zero_dict()     # p(class)\n",
        "    self.classes = classes                              # [class1, class2, ..]\n",
        "    self.word_probs = self.new_default_zero_dict()      # p(w|class)\n",
        "    #self.post_probs = {}      # p(class|w)\n",
        "    self.word_counts = self.new_default_zero_dict()     # {total:n1, class1: n2, ... }\n",
        "    self.tokenizer = tokenizer\n",
        "    self.use_smoothing = False\n",
        "\n",
        "    for word in vocab:\n",
        "      self.word_counts[word] = self.new_default_zero_dict()\n",
        "\n",
        "  def __default_zero__():\n",
        "    return 0\n",
        "  \n",
        "  def new_default_zero_dict(self):\n",
        "    return defaultdict(lambda: 0)\n",
        "\n",
        "  def count_frequencies(self, dataset):\n",
        "    for row in dataset:\n",
        "      words = self.tokenizer.tokenize(row['sentence'])\n",
        "      y = row['class']\n",
        "      self.class_counts[y] += 1\n",
        "\n",
        "      # count overall word frequency, and frequency per class (label)\n",
        "      for word in words:\n",
        "        self.word_counts[word]['total'] += 1\n",
        "        self.word_counts[word][y] += 1\n",
        "\n",
        "    for c in self.classes:\n",
        "      self.class_probs[c] = self.class_counts[c] / len(dataset)\n",
        "\n",
        "  # calculate probs from a training dataset\n",
        "  def train(self, dataset):\n",
        "    self.count_frequencies(dataset)\n",
        "\n",
        "    # Calculate\tConditional probability of all words based on category\n",
        "    num_words = len(self.word_counts)\n",
        "    for word in self.word_counts:\n",
        "      for c in self.classes:\n",
        "        #\tP(word|class)  = # of documents in category containing word / num of all documents in that category\n",
        "        word_cat_freq = self.word_counts[word][c]\n",
        "        key = word + '__' + str(c)\n",
        "        if self.use_smoothing:\n",
        "          self.word_probs[key] = (word_cat_freq + 1) / (self.class_counts[c] + num_words)\n",
        "        else:\n",
        "          self.word_probs[key] = word_cat_freq / self.class_counts[c]\n",
        "\n",
        "  # return most likely class for each document\n",
        "  def predict(self, dataset):\n",
        "    # foreach document:\n",
        "    #  foreach class, calculate p(class|tokens) = p(tokens|class)*p(class) / p(tokens)\n",
        "    # (since p(tokens) is effectively a constant scaling factor when comparing these, we can ignore it)\n",
        "    Y_hat = []\n",
        "    for row in dataset:\n",
        "      words = self.tokenizer.tokenize(row['sentence'])\n",
        "      probs = {}\n",
        "      for c in self.classes:\n",
        "        p = self.class_probs[c]\n",
        "        for w in words:\n",
        "          p *= self.word_probs[(w + '__' + str(c))]\n",
        "        probs[c] = p\n",
        "      \n",
        "      # predicted class is the one with max probability\n",
        "      Y_hat.append(max(probs, key=probs.get))\n",
        "      \n",
        "    return Y_hat\n",
        "\n",
        "  def get_predictors(self, arg_class, vocab_probs):\n",
        "    predictors = {}\n",
        "    for word in self.word_counts:\n",
        "      # p(class|word) = p(word|class)*p(class)/p(word)\n",
        "      if word in vocab_probs:\n",
        "        predictors[word] = self.word_probs[word + '__' + str(arg_class)] * self.class_probs[arg_class] / vocab_probs[word]\n",
        "      else:\n",
        "        predictors[word] = 0\n",
        "    return predictors\n",
        "\n"
      ],
      "metadata": {
        "id": "HX5XWxBB0zCe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "infile1 = pandas.read_json('dataset/Automotive_5.json', lines=True)\n",
        "infile2 = pandas.read_json('dataset/electronics_reviews_uniq.json', orient='records')\n",
        "infile3 = pandas.read_csv('dataset/Flipkart_Reviews - Electronics.csv')\n",
        "\n",
        "# Standardize column headings to: (class, review, review_title)\n",
        "infile1.rename(columns={'overall': 'class', 'reviewText': 'review', 'summary': 'review_title'}, inplace=True)\n",
        "infile2.rename(columns={'description': 'review', 'rating': 'class'}, inplace=True)\n",
        "infile3.rename(columns={'summary': 'review_title', 'rating': 'class'}, inplace=True)\n",
        "\n",
        "def format_data(df):\n",
        "  # convert all rating values to ints (1-5)\n",
        "  # todo: check for any invalid rating values\n",
        "  df = df.astype({\"class\": int})\n",
        "\n",
        "  # Combine the review with the summary (review title)\n",
        "  # ref: https://sparkbyexamples.com/pandas/pandas-combine-two-columns-of-text-in-dataframe/\n",
        "  df['sentence'] = df['review_title'] + \" \" + df['review']\n",
        "\n",
        "  # Remove unnecessary data columns (we just want [sentence, class])\n",
        "  return df[['sentence', 'class']]\n",
        "\n",
        "df1 = format_data(infile1)\n",
        "df2 = format_data(infile2)\n",
        "df3 = format_data(infile3)\n",
        "\n",
        "# preview each input file to ensure data looks correct\n",
        "print('File1 has', len(df1), 'rows')\n",
        "print(df1.head(5))\n",
        "print('File2 has', len(df2), 'rows')\n",
        "print(df2.head(5))\n",
        "print('File3 has', len(df3), 'rows')\n",
        "print(df3.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfR_MZ2LfYXN",
        "outputId": "598d5b46-adc4-49f4-bccc-2f03649697f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File1 has 20473 rows\n",
            "                                            sentence  class\n",
            "0  Work Well - Should Have Bought Longer Ones I n...      5\n",
            "1  Okay long cables These long cables work fine f...      4\n",
            "2  Looks and feels heavy Duty Can't comment much ...      5\n",
            "3  Excellent choice for Jumper Cables!!! I absolu...      5\n",
            "4  Excellent, High Quality Starter Cables I purch...      5\n",
            "File2 has 15166 rows\n",
            "                                            sentence  class\n",
            "0  Super! 1) Camera quality : Awesome ... it gene...      5\n",
            "1  Terrific purchase Awesome mobile , I got it wi...      5\n",
            "2  Delightful Honor 10 lite :Pros :-1) Cameras ar...      4\n",
            "3  Brilliant Superb camera, beautiful and slim de...      5\n",
            "4  Must buy! good and best mobil honor is best co...      5\n",
            "File3 has 9374 rows\n",
            "                                            sentence  class\n",
            "0  Terrific purchase 1-more flexible2-bass is ver...      5\n",
            "1  Terrific purchase Super sound and good looking...      5\n",
            "2  Super! Very much satisfied with the device at ...      5\n",
            "3  Super! Nice headphone, bass was very good and ...      5\n",
            "4  Terrific purchase Sound quality super battery ...      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merge the data\n",
        "all_data = pandas.concat([df1, df2, df3])\n",
        "\n",
        "# how many rows?\n",
        "print(len(all_data), 'total rows loaded')\n",
        "\n",
        "# throw out any empty reviews (see https://stackoverflow.com/a/56708633)\n",
        "all_data['sentence'].replace('', np.nan, inplace=True)\n",
        "all_data.dropna(subset=['sentence'], inplace=True)\n",
        "print(len(all_data), 'rows after removing empty reviews')\n",
        "\n",
        "# Convert to dict for easier processing later\n",
        "all_data = all_data.to_dict('records')\n",
        "\n",
        "classes = [1,2,3,4,5]\n",
        "\n",
        "split1 = int(.4*len(all_data))\n",
        "split2 = int(.7*len(all_data))\n",
        "traindata = all_data[:split1]\n",
        "devdata = all_data[split1:split2]\n",
        "testdata = all_data[split2:]\n",
        "\n",
        "print(len(traindata), 'training,', len(devdata), 'dev,', len(testdata), 'test rows loaded')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y-hN-NjLDg2",
        "outputId": "4b627af5-5069-4d44-d036-b261b66846f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45013 total rows loaded\n",
            "45013 rows after removing empty reviews\n",
            "18005 training, 13504 dev, 13504 test rows loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "tokenizer = MyTokenizer()\n",
        "\n",
        "for row in all_data:\n",
        "  row['tokens'] = tokenizer.tokenize(row['sentence'])\n",
        "  for tok in row['tokens']:\n",
        "    vocab.add(tok)\n",
        "\n",
        "# preview\n",
        "print('Found', len(vocab), 'words in vocabulary')\n",
        "for id,val in enumerate(vocab):\n",
        "  if id < 25:\n",
        "    print(val + \", \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX_Lls1Wq7Ji",
        "outputId": "f37d1830-b596-4f8f-8495-1a94363e15b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31120 words in vocabulary\n",
            "magicrat, \n",
            "chipset, \n",
            "havnt, \n",
            "1-ton, \n",
            "handyoveral, \n",
            "inventor, \n",
            "demegz, \n",
            "worsen, \n",
            "shiftn, \n",
            "ðŸ‘ðŸ», \n",
            "wi, \n",
            "aswc, \n",
            "sampler, \n",
            "dashcommand, \n",
            "merciless, \n",
            "dyna, \n",
            "adam, \n",
            "hting, \n",
            "raamaudio, \n",
            "installmi, \n",
            "rangeðŸ‘Œ, \n",
            "too-bad, \n",
            "procedur, \n",
            "proxim, \n",
            "46lbs, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = NaiveBayesClassifier(vocab, classes, tokenizer)\n",
        "model.train(traindata)"
      ],
      "metadata": {
        "id": "eoILzRWQtDqm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate probability of occurrance of each word\n",
        "vocab_probs = {}\n",
        "for word in vocab:\n",
        "  if (model.word_counts[word]['total'] > 0):    # if freq == 0 that means it did not appear in the training data\n",
        "    vocab_probs[word] = model.word_counts[word]['total'] / len(traindata)\n",
        "\n",
        "# preview: words with highest probability\n",
        "print('Top 5: words with highest probability')\n",
        "for w in sorted(vocab_probs, key=vocab_probs.get, reverse=True)[:8]:\n",
        "  print(w, vocab_probs[w])\n",
        "\n",
        "# preview: words with lowest probability\n",
        "print(\"\\n\",'Bottom 5: words with lowest probability')\n",
        "for w in sorted(vocab_probs, key=vocab_probs.get)[:8]:\n",
        "  print(w, vocab_probs[w])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFAOMotmw7OM",
        "outputId": "b5673e5d-5042-4eb9-b677-a8a9f6a1ebdd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5: words with highest probability\n",
            "use 0.4687031380172174\n",
            "work 0.3747292418772563\n",
            "great 0.3316856428769786\n",
            "n't 0.29919466814773676\n",
            "good 0.2788669813940572\n",
            "one 0.25276312135517914\n",
            "'s 0.24087753401832823\n",
            "veri 0.23848930852540962\n",
            "\n",
            " Bottom 5: words with lowest probability\n",
            "spendier 5.554012774229381e-05\n",
            "nippon 5.554012774229381e-05\n",
            "1040 5.554012774229381e-05\n",
            "14mm 5.554012774229381e-05\n",
            "bleed-back 5.554012774229381e-05\n",
            "dressing-soft 5.554012774229381e-05\n",
            "itinstal 5.554012774229381e-05\n",
            "6ga 5.554012774229381e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preview Conditional Probabilities (computed during training)\n",
        "print('Top 5 Conditional probability')\n",
        "for w in sorted(model.word_probs, key=model.word_probs.get, reverse=True)[:5]:\n",
        "  print(w, model.word_probs[w])\n",
        "\n",
        "print()\n",
        "print('Bottom 5 Conditional probability')\n",
        "for w in sorted(model.word_probs, key=model.word_probs.get)[:5]:\n",
        "  print(w, model.word_probs[w])\n",
        "\n",
        "# Preview class probabilities\n",
        "print()\n",
        "print('Class probabilities')\n",
        "for c in classes:\n",
        "  print(c, model.class_probs[c])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJihGMTy09Je",
        "outputId": "f91b714e-43fa-4bdf-b258-69ff4e62496b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Conditional probability\n",
            "use__2 0.4933837429111531\n",
            "n't__2 0.4820415879017013\n",
            "use__5 0.4721308845937926\n",
            "use__4 0.4615154237795747\n",
            "use__3 0.4607594936708861\n",
            "\n",
            "Bottom 5 Conditional probability\n",
            "children__1 0.0\n",
            "42-smd__1 0.0\n",
            "42-smd__2 0.0\n",
            "42-smd__3 0.0\n",
            "42-smd__4 0.0\n",
            "\n",
            "Class probabilities\n",
            "1 0.02682588169952791\n",
            "2 0.029380727575673424\n",
            "3 0.06581505137461816\n",
            "4 0.18544848653151902\n",
            "5 0.6925298528186615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try predicting labels for dev data\n",
        "Y_hat = model.predict(devdata)\n"
      ],
      "metadata": {
        "id": "YtIkSVK9-iLk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Responsibility predictions:', len([y for y in Y_hat if y == 'Responsibility']))\n",
        "#print('SoftSkill predictions:', len([y for y in Y_hat if y == 'SoftSkill']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9MQIr9DHEZL",
        "outputId": "f4c267f6-d064-4aa2-a55e-28f24a90f230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Responsibility predictions: 14045\n",
            "SoftSkill predictions: 3754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "def calculate_accuracy(Y, Y_hat):\n",
        "  num_correct = 0\n",
        "  index=0\n",
        "  if len(Y) != len(Y_hat):\n",
        "    print('data invalid length', len(Y), ':', len(Y_hat))\n",
        "    return False\n",
        "  for row in Y:\n",
        "    y_hat = Y_hat[index]\n",
        "    y = row['class']\n",
        "    num_correct += 1 if y_hat == y else 0\n",
        "    index += 1\n",
        "  return num_correct / len(Y)\n",
        "\n",
        "Y = devdata\n",
        "accuracy = calculate_accuracy(Y, Y_hat)\n",
        "\n",
        "print('Accuracy for Dev data:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se5s7pAhBT0z",
        "outputId": "734e40fa-738c-4200-b298-aa53ddd8404f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Dev data: 0.426614336492891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the result with smoothing\n",
        "model2 = NaiveBayesClassifier(vocab, classes, tokenizer)\n",
        "model2.use_smoothing = True\n",
        "model2.train(traindata)\n",
        "\n",
        "Y_hat = model2.predict(devdata)\n",
        "accuracy = calculate_accuracy(Y, Y_hat)\n",
        "\n",
        "print('Accuracy with smoothing:', accuracy)\n"
      ],
      "metadata": {
        "id": "0aJODZdgLNlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5277af17-168e-4a72-ed04-7848465ab7e7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with smoothing: 0.5896771327014217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive Top 10 words that predicts each class\n",
        "for c in classes:\n",
        "  print('Top 10 for category', c)\n",
        "  t2 = model.get_predictors(c, vocab_probs)\n",
        "  for w in sorted(t2, key=t2.get, reverse=True)[:10]:\n",
        "    print(w, t2[w])\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q12b4gkgMf1Y",
        "outputId": "5d657eda-50ed-496d-9a1e-bc5161d9fe49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 for category 1\n",
            "gue 1.0000000000000002\n",
            "crave 1.0000000000000002\n",
            "opti-len 1.0000000000000002\n",
            "knock-whackkveetch-chit-chit-crunk-snitbut 1.0000000000000002\n",
            "20ish 1.0000000000000002\n",
            "6-23-2013i 1.0000000000000002\n",
            "woman-own 1.0000000000000002\n",
            "abborb 1.0000000000000002\n",
            "petit 1.0000000000000002\n",
            "180f 1.0000000000000002\n",
            "\n",
            "Top 10 for category 2\n",
            "top-coat 1.0\n",
            "restrictionwat 1.0\n",
            "lene 1.0\n",
            "outsourc 1.0\n",
            "undepend 1.0\n",
            "loath 1.0\n",
            "okay-though 1.0\n",
            "falcon 1.0\n",
            "1216 1.0\n",
            "b0012fen0k 1.0\n",
            "\n",
            "Top 10 for category 3\n",
            "smartgaug 0.9999999999999999\n",
            "seme 0.9999999999999999\n",
            "re-hydr 0.9999999999999999\n",
            "conventionalanco 0.9999999999999999\n",
            "re-bend 0.9999999999999999\n",
            "drunken 0.9999999999999999\n",
            "well-us 0.9999999999999999\n",
            "overextend 0.9999999999999999\n",
            "durable- 0.9999999999999999\n",
            "anderson 0.9999999999999999\n",
            "\n",
            "Top 10 for category 4\n",
            "175w 1.0000000000000002\n",
            "cloths-it 1.0000000000000002\n",
            "hand-tighten 1.0000000000000002\n",
            "sensat 1.0000000000000002\n",
            "limp 1.0000000000000002\n",
            "drawbar 1.0000000000000002\n",
            "1-2-3 1.0000000000000002\n",
            "over-pr 1.0000000000000002\n",
            "velost 1.0000000000000002\n",
            "touchi 1.0000000000000002\n",
            "\n",
            "Top 10 for category 5\n",
            "vault 1.0000000000000002\n",
            "p3 1.0000000000000002\n",
            "transpond 1.0000000000000002\n",
            "thier 1.0000000000000002\n",
            "pre-filt 1.0000000000000002\n",
            "tlc 1.0000000000000002\n",
            "dual-act 1.0000000000000002\n",
            "drape 1.0000000000000002\n",
            "boiler 1.0000000000000002\n",
            "upstream 1.0000000000000002\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with Decision Tree Approach\n",
        "# ref: https://www.codementor.io/blog/text-classification-6mmol0q8oj\n",
        "from sklearn import tree\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = [row.get('sentence') for row in traindata]\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "y_train = [row.get('class') for row in traindata]\n",
        "\n",
        "X_val = [row.get('sentence') for row in devdata]\n",
        "X_val = vectorizer.transform(X_val)\n",
        "y_val = [row.get('class') for row in devdata]\n"
      ],
      "metadata": {
        "id": "Un-_NUKQGdzL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = tree.DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtzj_2NaNfpq",
        "outputId": "35eb3dbb-b095-4b5e-b2fe-e0edcbf70780"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = dt.predict(X_val)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "print(confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBXPnv3gH3rt",
        "outputId": "dd4f4818-5486-4f99-cc3f-937834996e29"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.70      0.18      0.28       943\n",
            "           2       0.12      0.04      0.06       390\n",
            "           3       0.32      0.14      0.20      1067\n",
            "           4       0.42      0.18      0.25      3084\n",
            "           5       0.65      0.91      0.76      8020\n",
            "\n",
            "    accuracy                           0.61     13504\n",
            "   macro avg       0.44      0.29      0.31     13504\n",
            "weighted avg       0.56      0.61      0.54     13504\n",
            "\n",
            "[[ 166   28   39   61  649]\n",
            " [  12   15   34   54  275]\n",
            " [   6   13  154  184  710]\n",
            " [  14   32  119  550 2369]\n",
            " [  40   36  137  476 7331]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "# ref: https://www.codementor.io/blog/text-classification-6mmol0q8oj\n",
        "\n",
        "svm = LinearSVC()\n",
        "_ = svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye-vQLQxMRrY",
        "outputId": "59b2e8c6-e735-4d7b-8f36-576e639c75a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.81      0.34      0.47       943\n",
            "           2       0.11      0.03      0.05       390\n",
            "           3       0.48      0.18      0.26      1067\n",
            "           4       0.43      0.13      0.20      3084\n",
            "           5       0.65      0.95      0.77      8020\n",
            "\n",
            "    accuracy                           0.63     13504\n",
            "   macro avg       0.50      0.33      0.35     13504\n",
            "weighted avg       0.58      0.63      0.56     13504\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "CSV reader example code:\\\n",
        "https://realpython.com/python-csv/\n",
        "\n",
        "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "\n",
        "https://www.geeksforgeeks.org/python-stemming-words-with-nltk/?ref=lbp\n",
        "\n",
        "https://stackoverflow.com/questions/29314033/drop-rows-containing-empty-cells-from-a-pandas-dataframe\n",
        "\n",
        "https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
        "\n",
        "https://towardsdatascience.com/heres-the-most-efficient-way-to-iterate-through-your-pandas-dataframe-4dad88ac92ee"
      ],
      "metadata": {
        "id": "mt01xYnnkhzr"
      }
    }
  ]
}