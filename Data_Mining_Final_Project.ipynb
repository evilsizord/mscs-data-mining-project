{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKVWBBhVYzbkA5dajajeH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evilsizord/mscs-data-mining-project/blob/main/Data_Mining_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Review Classification\n",
        "Final Project \\\n",
        "CSE 5334 - Data Mining \\\n",
        "Daniel Evilsizor \\\n",
        "November 13, 2022"
      ],
      "metadata": {
        "id": "XQM0pRlgfLq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data from Kaggle.com\n",
        "\n",
        "# NOTE: Requires you to have a Kaggle.com account. From your account you can generate an API key.\n",
        "# It will be provided in kaggle.json. Upload the JSON file to this project BEFORE RUNNING.\n",
        "\n",
        "# src: https://www.kaggle.com/general/74235\n",
        "\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download vivekgediya/ecommerce-product-review-data\n",
        "! mkdir dataset\n",
        "! unzip ecommerce-product-review-data.zip -d dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhmeJ5GQfVE3",
        "outputId": "067a8f88-2383-4b98-b39d-aaad7b03950b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ecommerce-product-review-data.zip to /content\n",
            "\r  0% 0.00/8.55M [00:00<?, ?B/s]\r 94% 8.00M/8.55M [00:00<00:00, 82.6MB/s]\n",
            "\r100% 8.55M/8.55M [00:00<00:00, 86.7MB/s]\n",
            "Archive:  ecommerce-product-review-data.zip\n",
            "  inflating: dataset/Automotive_5.json  \n",
            "  inflating: dataset/Flipkart_Reviews - Electronics.csv  \n",
            "  inflating: dataset/Product Review Data.csv  \n",
            "  inflating: dataset/Product Review Large Data.csv  \n",
            "  inflating: dataset/electronics_reviews_uniq.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load all necessary libraries\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import pandas\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import datasets, layers, models, losses\n",
        "from sklearn import tree\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
        "\n",
        "nltk.download('punkt')  # needed for tokenizer\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia4MspXxfWl7",
        "outputId": "7e41cee2-71f9-4f27-b0d5-34e983a8763d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTokenizer:\n",
        "  def __init__(self):\n",
        "    self.stemmer = SnowballStemmer(\"english\")\n",
        "    self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  def tokenize(self, doc):\n",
        "    # strip punctuation, it's not needed\n",
        "    # ref: https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string\n",
        "    chars = \"/*[]()>#+,.!;:?\"\n",
        "    for c in chars:\n",
        "        doc = doc.replace(c, \" \")\n",
        "    tokens = word_tokenize(doc)\n",
        "    # remove stop words\n",
        "    # ref: https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
        "    # Also, convert to a set so we only get unique words per row (no duplicates)\n",
        "    words = set()\n",
        "    for tok in tokens:\n",
        "      word = self.stemmer.stem(tok)\n",
        "      if not word.lower() in self.stop_words:\n",
        "        words.add(word)\n",
        "    return words\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "\n",
        "  def __init__(self, vocab, classes, tokenizer):\n",
        "    self.class_counts = self.new_default_zero_dict()\n",
        "    self.class_probs = self.new_default_zero_dict()     # p(class)\n",
        "    self.classes = classes                              # [class1, class2, ..]\n",
        "    self.word_probs = self.new_default_zero_dict()      # p(w|class)\n",
        "    #self.post_probs = {}      # p(class|w)\n",
        "    self.word_counts = self.new_default_zero_dict()     # {total:n1, class1: n2, ... }\n",
        "    self.tokenizer = tokenizer\n",
        "    self.use_smoothing = False\n",
        "\n",
        "    for word in vocab:\n",
        "      self.word_counts[word] = self.new_default_zero_dict()\n",
        "\n",
        "  def __default_zero__():\n",
        "    return 0\n",
        "  \n",
        "  def new_default_zero_dict(self):\n",
        "    return defaultdict(lambda: 0)\n",
        "\n",
        "  def count_frequencies(self, dataset):\n",
        "    for row in dataset:\n",
        "      words = self.tokenizer.tokenize(row['sentence'])\n",
        "      y = row['class']\n",
        "      self.class_counts[y] += 1\n",
        "\n",
        "      # count overall word frequency, and frequency per class (label)\n",
        "      for word in words:\n",
        "        self.word_counts[word]['total'] += 1\n",
        "        self.word_counts[word][y] += 1\n",
        "\n",
        "    for c in self.classes:\n",
        "      self.class_probs[c] = self.class_counts[c] / len(dataset)\n",
        "\n",
        "  # calculate probs from a training dataset\n",
        "  def train(self, dataset):\n",
        "    self.count_frequencies(dataset)\n",
        "\n",
        "    # Calculate\tConditional probability of all words based on category\n",
        "    num_words = len(self.word_counts)\n",
        "    for word in self.word_counts:\n",
        "      for c in self.classes:\n",
        "        #\tP(word|class)  = # of documents in category containing word / num of all documents in that category\n",
        "        word_cat_freq = self.word_counts[word][c]\n",
        "        key = word + '__' + str(c)\n",
        "        if self.use_smoothing:\n",
        "          self.word_probs[key] = (word_cat_freq + 1) / (self.class_counts[c] + num_words)\n",
        "        else:\n",
        "          self.word_probs[key] = word_cat_freq / self.class_counts[c]\n",
        "\n",
        "  # return most likely class for each document\n",
        "  def predict(self, dataset):\n",
        "    # foreach document:\n",
        "    #  foreach class, calculate p(class|tokens) = p(tokens|class)*p(class) / p(tokens)\n",
        "    # (since p(tokens) is effectively a constant scaling factor when comparing these, we can ignore it)\n",
        "    Y_hat = []\n",
        "    for row in dataset:\n",
        "      words = self.tokenizer.tokenize(row['sentence'])\n",
        "      probs = {}\n",
        "      for c in self.classes:\n",
        "        p = self.class_probs[c]\n",
        "        for w in words:\n",
        "          p *= self.word_probs[(w + '__' + str(c))]\n",
        "        probs[c] = p\n",
        "      \n",
        "      # predicted class is the one with max probability\n",
        "      Y_hat.append(max(probs, key=probs.get))\n",
        "      \n",
        "    return Y_hat\n",
        "\n",
        "  def get_predictors(self, arg_class, vocab_probs):\n",
        "    predictors = {}\n",
        "    for word in self.word_counts:\n",
        "      # p(class|word) = p(word|class)*p(class)/p(word)\n",
        "      if word in vocab_probs:\n",
        "        predictors[word] = self.word_probs[word + '__' + str(arg_class)] * self.class_probs[arg_class] / vocab_probs[word]\n",
        "      else:\n",
        "        predictors[word] = 0\n",
        "    return predictors\n",
        "\n"
      ],
      "metadata": {
        "id": "HX5XWxBB0zCe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "infile1 = pandas.read_json('dataset/Automotive_5.json', lines=True)\n",
        "infile2 = pandas.read_json('dataset/electronics_reviews_uniq.json', orient='records')\n",
        "infile3 = pandas.read_csv('dataset/Flipkart_Reviews - Electronics.csv')\n",
        "\n",
        "# Standardize column headings to: (class, review, review_title)\n",
        "infile1.rename(columns={'overall': 'class', 'reviewText': 'review', 'summary': 'review_title'}, inplace=True)\n",
        "infile2.rename(columns={'description': 'review', 'rating': 'class'}, inplace=True)\n",
        "infile3.rename(columns={'summary': 'review_title', 'rating': 'class'}, inplace=True)\n",
        "\n",
        "def format_data(df):\n",
        "  # convert all rating values to ints (1-5)\n",
        "  # todo: check for any invalid rating values\n",
        "  df = df.astype({\"class\": int})\n",
        "\n",
        "  # Combine the review with the summary (review title)\n",
        "  # ref: https://sparkbyexamples.com/pandas/pandas-combine-two-columns-of-text-in-dataframe/\n",
        "  df['sentence'] = df['review_title'] + \" \" + df['review']\n",
        "\n",
        "  # Remove unnecessary data columns (we just want [sentence, class])\n",
        "  return df[['sentence', 'class']]\n",
        "\n",
        "df1 = format_data(infile1)\n",
        "df2 = format_data(infile2)\n",
        "df3 = format_data(infile3)\n",
        "\n",
        "# preview each input file to ensure data looks correct\n",
        "print('File1 has', len(df1), 'rows')\n",
        "print(df1.head(5))\n",
        "print('File2 has', len(df2), 'rows')\n",
        "print(df2.head(5))\n",
        "print('File3 has', len(df3), 'rows')\n",
        "print(df3.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfR_MZ2LfYXN",
        "outputId": "0a0473b9-3f38-4364-81be-6199d284270c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File1 has 20473 rows\n",
            "                                            sentence  class\n",
            "0  Work Well - Should Have Bought Longer Ones I n...      5\n",
            "1  Okay long cables These long cables work fine f...      4\n",
            "2  Looks and feels heavy Duty Can't comment much ...      5\n",
            "3  Excellent choice for Jumper Cables!!! I absolu...      5\n",
            "4  Excellent, High Quality Starter Cables I purch...      5\n",
            "File2 has 15166 rows\n",
            "                                            sentence  class\n",
            "0  Super! 1) Camera quality : Awesome ... it gene...      5\n",
            "1  Terrific purchase Awesome mobile , I got it wi...      5\n",
            "2  Delightful Honor 10 lite :Pros :-1) Cameras ar...      4\n",
            "3  Brilliant Superb camera, beautiful and slim de...      5\n",
            "4  Must buy! good and best mobil honor is best co...      5\n",
            "File3 has 9374 rows\n",
            "                                            sentence  class\n",
            "0  Terrific purchase 1-more flexible2-bass is ver...      5\n",
            "1  Terrific purchase Super sound and good looking...      5\n",
            "2  Super! Very much satisfied with the device at ...      5\n",
            "3  Super! Nice headphone, bass was very good and ...      5\n",
            "4  Terrific purchase Sound quality super battery ...      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merge the data\n",
        "all_data = pandas.concat([df1, df2, df3])\n",
        "\n",
        "# how many rows?\n",
        "print(len(all_data), 'total rows loaded')\n",
        "\n",
        "# throw out any empty reviews (see https://stackoverflow.com/a/56708633)\n",
        "all_data['sentence'].replace('', np.nan, inplace=True)\n",
        "all_data.dropna(subset=['sentence'], inplace=True)\n",
        "print(len(all_data), 'rows after removing empty reviews')\n",
        "\n",
        "# Convert to dict for easier processing later\n",
        "all_data = all_data.to_dict('records')\n",
        "\n",
        "classes = [1,2,3,4,5]\n",
        "\n",
        "split1 = int(.8*len(all_data))\n",
        "split2 = int(.9*len(all_data))\n",
        "traindata = all_data[:split1]\n",
        "devdata = all_data[split1:split2]\n",
        "testdata = all_data[split2:]\n",
        "\n",
        "print(len(traindata), 'training,', len(devdata), 'dev,', len(testdata), 'test rows loaded')\n",
        "\n",
        "X_train = [row.get('sentence') for row in traindata]\n",
        "y_train = [row.get('class') for row in traindata]\n",
        "\n",
        "X_val = [row.get('sentence') for row in devdata]\n",
        "y_val = [row.get('class') for row in devdata]\n",
        "\n",
        "X_test = [row.get('sentence') for row in testdata]\n",
        "y_test = [row.get('class') for row in testdata]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y-hN-NjLDg2",
        "outputId": "5f8cf385-7a62-4fab-ec6c-d8f5d553756d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45013 total rows loaded\n",
            "45013 rows after removing empty reviews\n",
            "36010 training, 4501 dev, 4502 test rows loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate vocabulary\n",
        "vocab = set()\n",
        "tokenizer = MyTokenizer()\n",
        "\n",
        "for row in all_data:\n",
        "  row['tokens'] = tokenizer.tokenize(row['sentence'])\n",
        "  for tok in row['tokens']:\n",
        "    vocab.add(tok)\n",
        "\n",
        "# preview\n",
        "print('Found', len(vocab), 'words in vocabulary')\n",
        "for id,val in enumerate(vocab):\n",
        "  if id < 25:\n",
        "    print(val + \", \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX_Lls1Wq7Ji",
        "outputId": "b6724ccf-e09a-44b6-851a-fab974391981"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31120 words in vocabulary\n",
            "warrantyupd, \n",
            "pursu, \n",
            "soun, \n",
            "crappi, \n",
            "needle-nos, \n",
            "non-incandesc, \n",
            "rollback, \n",
            "rangeyou, \n",
            "-sound, \n",
            "poduct, \n",
            "vortex, \n",
            "jod, \n",
            "pre-mold, \n",
            "headlin, \n",
            "00272-sllc2, \n",
            "-best, \n",
            "maalibu, \n",
            "non-obd-ii, \n",
            "clayblock, \n",
            "betweenbear, \n",
            "rewiew, \n",
            "looki, \n",
            "venthos, \n",
            "previousali, \n",
            "dy, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "nbc = NaiveBayesClassifier(vocab, classes, tokenizer)\n",
        "nbc.train(traindata)"
      ],
      "metadata": {
        "id": "eoILzRWQtDqm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate probability of occurrance of each word\n",
        "vocab_probs = {}\n",
        "for word in vocab:\n",
        "  if (nbc.word_counts[word]['total'] > 0):    # if freq == 0 that means it did not appear in the training data\n",
        "    vocab_probs[word] = nbc.word_counts[word]['total'] / len(traindata)\n",
        "\n",
        "# preview: words with highest probability\n",
        "print('Top 5: words with highest probability')\n",
        "for w in sorted(vocab_probs, key=vocab_probs.get, reverse=True)[:8]:\n",
        "  print(w, vocab_probs[w])\n",
        "\n",
        "# preview: words with lowest probability\n",
        "print(\"\\n\",'Bottom 5: words with lowest probability')\n",
        "for w in sorted(vocab_probs, key=vocab_probs.get)[:8]:\n",
        "  print(w, vocab_probs[w])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFAOMotmw7OM",
        "outputId": "31f671d9-ae3f-4182-ffde-ec3173f340ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5: words with highest probability\n",
            "good 0.33832268814218275\n",
            "use 0.2915301305193002\n",
            "product 0.260483199111358\n",
            "work 0.22826992502082755\n",
            "great 0.21727297972785337\n",
            "veri 0.21627325742849207\n",
            "n't 0.1998333796167731\n",
            "nice 0.17056373229658428\n",
            "\n",
            " Bottom 5: words with lowest probability\n",
            "warrantyupd 2.7770063871146904e-05\n",
            "non-incandesc 2.7770063871146904e-05\n",
            "rollback 2.7770063871146904e-05\n",
            "jod 2.7770063871146904e-05\n",
            "pre-mold 2.7770063871146904e-05\n",
            "00272-sllc2 2.7770063871146904e-05\n",
            "maalibu 2.7770063871146904e-05\n",
            "non-obd-ii 2.7770063871146904e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preview Conditional Probabilities (computed during training)\n",
        "print('Top 5 Conditional probability')\n",
        "for w in sorted(nbc.word_probs, key=nbc.word_probs.get, reverse=True)[:5]:\n",
        "  print(w, nbc.word_probs[w])\n",
        "\n",
        "print()\n",
        "print('Bottom 5 Conditional probability')\n",
        "for w in sorted(nbc.word_probs, key=nbc.word_probs.get)[:5]:\n",
        "  print(w, nbc.word_probs[w])\n",
        "\n",
        "# Preview class probabilities\n",
        "print()\n",
        "print('Class probabilities')\n",
        "for c in classes:\n",
        "  print(c, nbc.class_probs[c])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJihGMTy09Je",
        "outputId": "cfe23220-a3a5-4ca7-e0af-8e105bf7c187"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Conditional probability\n",
            "good__4 0.5238424326192122\n",
            "good__3 0.39270130900436334\n",
            "n't__2 0.3403041825095057\n",
            "n't__1 0.32483156881616937\n",
            "use__2 0.31749049429657794\n",
            "\n",
            "Bottom 5 Conditional probability\n",
            "warrantyupd__1 0.0\n",
            "warrantyupd__2 0.0\n",
            "warrantyupd__3 0.0\n",
            "warrantyupd__4 0.0\n",
            "pursu__1 0.0\n",
            "\n",
            "Class probabilities\n",
            "1 0.057706192724243266\n",
            "2 0.02921410719244654\n",
            "3 0.07000833101916135\n",
            "4 0.20091641210774785\n",
            "5 0.642154956956401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try predicting labels for dev data\n",
        "Y_hat = nbc.predict(devdata)"
      ],
      "metadata": {
        "id": "YtIkSVK9-iLk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Responsibility predictions:', len([y for y in Y_hat if y == 'Responsibility']))\n",
        "#print('SoftSkill predictions:', len([y for y in Y_hat if y == 'SoftSkill']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9MQIr9DHEZL",
        "outputId": "f4c267f6-d064-4aa2-a55e-28f24a90f230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Responsibility predictions: 14045\n",
            "SoftSkill predictions: 3754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "def calculate_accuracy(Y, Y_hat):\n",
        "  num_correct = 0\n",
        "  index=0\n",
        "  if len(Y) != len(Y_hat):\n",
        "    print('data invalid length', len(Y), ':', len(Y_hat))\n",
        "    return False\n",
        "  for row in Y:\n",
        "    y_hat = Y_hat[index]\n",
        "    y = row['class']\n",
        "    num_correct += 1 if y_hat == y else 0\n",
        "    index += 1\n",
        "  return num_correct / len(Y)\n",
        "\n",
        "Y = devdata\n",
        "accuracy = calculate_accuracy(Y, Y_hat)\n",
        "\n",
        "print('Accuracy for Dev data:', accuracy)\n",
        "\n",
        "print(classification_report(Y, Y_hat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se5s7pAhBT0z",
        "outputId": "5753954c-7316-4f0d-c11b-c72b8d295555"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Dev data: 0.5014441235281049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the result with smoothing\n",
        "nbc2 = NaiveBayesClassifier(vocab, classes, tokenizer)\n",
        "nbc2.use_smoothing = True\n",
        "nbc2.train(traindata)\n",
        "\n",
        "Y_hat = nbc2.predict(devdata)\n",
        "accuracy = calculate_accuracy(Y, Y_hat)\n",
        "\n",
        "print('Accuracy with smoothing:', accuracy)\n",
        "\n",
        "print(classification_report(Y, Y_hat))\n"
      ],
      "metadata": {
        "id": "0aJODZdgLNlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d1f08c-125c-424f-da27-1241ed1e3e2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with smoothing: 0.6149744501221951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive Top 10 words that predicts each class\n",
        "\"\"\"\n",
        "for c in classes:\n",
        "  print('Top 10 for category', c)\n",
        "  t2 = nbc2.get_predictors(c, vocab_probs)\n",
        "  for w in sorted(t2, key=t2.get, reverse=True)[:10]:\n",
        "    print(w, t2[w])\n",
        "  print()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q12b4gkgMf1Y",
        "outputId": "7e5abe3b-be88-482c-a5b1-d761977d1ffc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 for category 1\n",
            "638v 1.0\n",
            "mobilenot 1.0\n",
            "kark 1.0\n",
            "refundbl 1.0\n",
            "rounded-edg 1.0\n",
            "9-20-2013i 1.0\n",
            "pluber 1.0\n",
            "updatedpoor 1.0\n",
            "7c◆no 1.0\n",
            "non-tint 1.0\n",
            "\n",
            "Top 10 for category 2\n",
            "known-compat 0.9999999999999999\n",
            "220laptop 0.9999999999999999\n",
            "poor-fit 0.9999999999999999\n",
            "longtim 0.9999999999999999\n",
            "indid 0.9999999999999999\n",
            "found-th 0.9999999999999999\n",
            "value-ad 0.9999999999999999\n",
            "seer 0.9999999999999999\n",
            "👎was 0.9999999999999999\n",
            "bulbous 0.9999999999999999\n",
            "\n",
            "Top 10 for category 3\n",
            "msrp 1.0000000000000002\n",
            "ironi 1.0000000000000002\n",
            "3-star 1.0000000000000002\n",
            "deviceeasi 1.0\n",
            "recommendedupd 1.0\n",
            "plx 1.0\n",
            "mosquito 1.0\n",
            "performance👎batteri 1.0\n",
            "fingerprintsoth 1.0\n",
            "poorboy 1.0\n",
            "\n",
            "Top 10 for category 4\n",
            "rollback 1.0000000000000002\n",
            "00272-sllc2 1.0000000000000002\n",
            "betweenbear 1.0000000000000002\n",
            "looki 1.0000000000000002\n",
            "venthos 1.0000000000000002\n",
            "fubar 1.0000000000000002\n",
            "alpinestar 1.0000000000000002\n",
            "lubricantthin 1.0000000000000002\n",
            "mini-flush 1.0000000000000002\n",
            "batterythat 1.0000000000000002\n",
            "\n",
            "Top 10 for category 5\n",
            "warrantyupd 1.0\n",
            "non-incandesc 1.0\n",
            "jod 1.0\n",
            "pre-mold 1.0\n",
            "headlin 1.0\n",
            "maalibu 1.0\n",
            "non-obd-ii 1.0\n",
            "clayblock 1.0\n",
            "tank1 1.0\n",
            "20111 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with Decision Tree Approach\n",
        "# ref: https://www.codementor.io/blog/text-classification-6mmol0q8oj\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train_v = vectorizer.fit_transform(X_train)\n",
        "y_train_v = vectorizer.transform(y_train)"
      ],
      "metadata": {
        "id": "Un-_NUKQGdzL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build decision tree based on training data\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "dt.fit(X_train_v, y_train_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtzj_2NaNfpq",
        "outputId": "866479cf-5980-448a-a735-3639ba2383f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the decision tree\n",
        "X_val_v = vectorizer.transform(X_val)\n",
        "y_pred = dt.predict(X_val_v)\n",
        "\n",
        "print(classification_report(y_val, y_pred))\n",
        "#print(confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBXPnv3gH3rt",
        "outputId": "20750e94-88cb-4b5d-be8f-aecb922e4d3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.79      0.84       526\n",
            "           2       0.51      0.57      0.54       126\n",
            "           3       0.41      0.39      0.40       230\n",
            "           4       0.55      0.59      0.57       898\n",
            "           5       0.83      0.83      0.83      2721\n",
            "\n",
            "    accuracy                           0.75      4501\n",
            "   macro avg       0.64      0.63      0.64      4501\n",
            "weighted avg       0.75      0.75      0.75      4501\n",
            "\n",
            "[[ 414   25   16   20   51]\n",
            " [  13   72    8   11   22]\n",
            " [   4   12   89   51   74]\n",
            " [   7    6   30  527  328]\n",
            " [  17   25   74  344 2261]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "# ref: https://www.codementor.io/blog/text-classification-6mmol0q8oj\n",
        "\n",
        "svm = LinearSVC()\n",
        "_ = svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye-vQLQxMRrY",
        "outputId": "a5ebb719-ac54-425a-e557-0b381fea8e41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.95      0.96       526\n",
            "           2       0.71      0.75      0.73       126\n",
            "           3       0.62      0.51      0.56       230\n",
            "           4       0.72      0.63      0.67       898\n",
            "           5       0.88      0.93      0.91      2721\n",
            "\n",
            "    accuracy                           0.85      4501\n",
            "   macro avg       0.78      0.76      0.77      4501\n",
            "weighted avg       0.84      0.85      0.84      4501\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NN\n",
        "# ref: https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "raw_train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "raw_train_ds = raw_train_ds.batch(batch_size)\n",
        "\n",
        "raw_val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "raw_val_ds = raw_val_ds.batch(batch_size)\n",
        "\n",
        "raw_test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "raw_test_ds = raw_test_ds.batch(batch_size)"
      ],
      "metadata": {
        "id": "s9ewl-tjUGun"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview train data\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(\"Sentence\", text_batch.numpy()[i])\n",
        "    print(\"Rating\", label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdXoQSIwYKdl",
        "outputId": "b08a1317-fcb3-4a71-e19c-b625a9276a06"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence b\"Work Well - Should Have Bought Longer Ones I needed a set of jumper cables for my new car and these had good reviews and were at a good price.  They have been used a few times already and do what they are supposed to - no complaints there.What I will say is that 12 feet really isn't an ideal length.  Sure, if you pull up front bumper to front bumper they are plenty long, but a lot of times you will be beside another car or can't get really close.  Because of this, I would recommend something a little longer than 12'.Great brand - get 16' version though.\"\n",
            "Rating 5\n",
            "Sentence b'Okay long cables These long cables work fine for my truck, but the quality seems a little on the shabby side. For the money I was not expecting 200 dollar snap-on jumper cables but these seem more like what you would see at a chinese knock off shop like harbor freight for 30 bucks.'\n",
            "Rating 4\n",
            "Sentence b\"Looks and feels heavy Duty Can't comment much on these since they have not yet been used (I will come back and update my review is I find any issues after first use) ... but they are built solid, nice tough big hard clamps and love having a long cable so I never have to move cars around or anything if needed. I bought these to have in my new truck since you always need cables ... but another reason is for when I tow my travel trailer and we run the batteries with no shore power they may die after a couple days ... if you are in need a quick small recharge they are the perfect length to pop my hood, run the cables to the back of the truck and hook up to the batteries that are on the tongue of my travel trailer ... running the truck for 30-45 minutes with this nice large gauge wire connected from my battery tot he trailer battery will give me a bit of a charge if ever in a pinch and I have no shore power, solar, or generator to recharge.Bought the bucket boss 06009 jumper cable bag and it fit this 25 footer Perfectly!!!It has no use and is a waste of money right now ... but will EASILY pay for itself the first time you need it ... always be prepared! :)\"\n",
            "Rating 5\n",
            "Sentence b'Excellent choice for Jumper Cables!!! I absolutley love Amazon!!!  For the price of a set of cheap Booster/Jumper Cables in a brick and morter store, you can buy extra long and heavy duty jumpers!  First off, don\\'t be the person that not only needs to ask a kind passer-by for a \"jump\" but also if they have jumper cables.  It\\'s MUCH easier to get a jump start if you have your own cables.Next lets talk about sizing.  Having the longest cable possible is a major plus if your car is parked up against something like a pole or wall, or even parked on a one way street.  The \"booster car\" (the car w/o a dead battery) can pull in close enough to use the cables without having to manuver into some akward position.  Or better yet, you won\\'t have to push your vehicle into a position to be jumped.  If your diving a normal sized car they can even pull in behind you to jump you!  Or if their vehicle is the shorter of the two, they could pull in front.  Now how about gauge?  For those who aren\\'t electricians or engineers, as the number value of gauge decreases, the size and capacity of the cable increases.  So for example 6-ga has about twice as much copper wire as 8-ga, and 4-ga is a about twice as much as 6-ga, so on and so on.  That doesn\\'t mean you double the capacity of wire with every two numbers though.  4-gauge may sound like a lot but it really isn\\'t.The technical stuff.  Don\\'t worry if this doesn\\'t make sense, you don\\'t really need to know all of this, but I\\'m trying to make a point as to why to get the longest and heavist cables you can.  I\\'m not an electrican but I\\'ve worked with it long enough to know a few general rules of thumb.  First of the enemy of electricty is resistance.  Resistance ultimately determines the load a particular wire can carry.  Five things factor into resistance.  Temperature, conductor, voltage, load, and distance.  Temp we can\\'t control, so we have to assume the worst (HOT)...or in other words need heavier cable.  Conductor should be copper, or at least a high purity mix of copper...good there.  Voltage...12VDC is a low voltage system...probably the biggest problem we face.  Since is a low voltage system we have to contend with voltage drop as it travels over the wire.  The higher the voltage, the longer it can run with acceptable levels of voltage drop over a particular gauge wire...so again since it\\'s low voltage need heavier gauge wire.  Load (Amperage)...it\\'s a car starter so it takes A LOT of power.  The voltage is a fixed number (more or less) so Amperage is determined by the size of the engine your trying to start.  And you\\'d be surprised how high that can be.  Even a small engine can be well over 300 Cranking Amps.  That said even a \"dead\" battery will put out some power.  As for distance...this cables selling point is also a draw back.  25\\' is LONG run for 12VDC.  So like I said in the last line 4-ga isn\\'t as much as you think.  In fact, if you have an engine over 6-liters you should really consider 2-ga for this distance.  So summing up in layman\\'s terms...considering the hot temperatures outside, the fact that it\\'s 12VDC and we need a long cable, and that we\\'re trying to start a car engine (which takes a LOT of power) these are the ONLY cables I could reccomend for anyone driving a small car up to a light truck.Lets talk about how to properly start a car with a drained battery.  Make sure the LAST connection make is the NEG clamp to the \"dead\" car on a frame ground (something metal and NOT MOVING or GOING TO MOVE in the engine compartment.)  Don\\'t attach it directly to the battery\\'s NEG post.  Next lightly rev the engine of the running vehicle for 3 to 5 minutes.  This will put a bit of a surface charge on the dead battery.  Then attempt to start the car.  As soon as it starts remove the cables (starting with the NEG on the car being jumped.)  If you ever have a dead battery for any reason...take it to an auto parts store as soon as possible and have it tested to make sure it still holds a proper charge.  Most places do it for free.  NEVER touch the clamps together once connected to a battery!!!  Doing so could damage you vehicles electrical system, and/or cause fire, burns, explosion of the battery, damages to your cables...A couple of final thoughts.  Protect your investment.  I bought a cable bag that works really well to keep them from getting tangled with everything in my trunk.  Also keep the twist ties that come with them which will keep them organized inside the cable bag.  And once used, take them home and clean and dry them before storing them once again.  The oils inside of engine compartments can be corrosive to rubber jackets and copper contacts.  These cables are more than worth they weight in gold!  They are inexpensive and top notch quality!'\n",
            "Rating 5\n",
            "Sentence b\"Excellent, High Quality Starter Cables I purchased the 12' feet long cable set and they arrived in a retail cardboard box with handle, which could be used as a storage and carrying case for these cables, although I will not need to store them with the carry box.  These are high quality long cables of high grade materials and I believe worth the price I paid for them.  They will store in the back seat storage compartment of my truck easily.  Recomend.\"\n",
            "Rating 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n"
      ],
      "metadata": {
        "id": "FyMwlZavY_Kb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview a vectorized item from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", first_label)\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP1sZW6LZ5cj",
        "outputId": "6ddb58d9-405c-4644-c60b-21d6801ada0a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b\"Work Well - Should Have Bought Longer Ones I needed a set of jumper cables for my new car and these had good reviews and were at a good price.  They have been used a few times already and do what they are supposed to - no complaints there.What I will say is that 12 feet really isn't an ideal length.  Sure, if you pull up front bumper to front bumper they are plenty long, but a lot of times you will be beside another car or can't get really close.  Because of this, I would recommend something a little longer than 12'.Great brand - get 16' version though.\", shape=(), dtype=string)\n",
            "Label tf.Tensor(5, shape=(), dtype=int32)\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[  72,   44,  142,   21,  104,  330,  331,    6,  227,    5,  219,\n",
            "          10, 2057,  734,   11,   14,  111,   33,    3,   34,   59,   18,\n",
            "         380,    3,  137,   42,    5,   18,   65,   31,   21,  112,   54,\n",
            "           5,  153,  309,  509,    3,   62,   83,   31,   24,  727,    4,\n",
            "          52, 1264,    1,    6,   39,  192,    8,   15,  450, 1145,   74,\n",
            "         421,   63, 1931,  774,  135,   36,   16,  545,   48,  248,  770,\n",
            "           4,  248,  770,   31,   24,  925,  109,   20,    5,  164,   10,\n",
            "         309,   16,   39,   30, 5245,  206,   33,   40,  202,   57,   74,\n",
            "         628,  101,   10,    9,    6,   60,  136,  269,    5,   90,  330,\n",
            "          55,    1,  338,   57, 1048,  760,  270,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=5>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the TextVectorization layer  to datasets\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# configure for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "r3QZsW1RaiVi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a model\n",
        "embedding_dim = 16\n",
        "\n",
        "ann = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "ann.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERbNoxDbar2Z",
        "outputId": "80c4a8c6-9461-4879-f36a-2ecf31032d21"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          160016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 16)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ann.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
        "\n",
        "# train the model\n",
        "epochs = 10\n",
        "#history = ann.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
        "history = ann.fit(train_ds, epochs=epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "5wNLhEiJazh2",
        "outputId": "0c0d7c2d-ed2f-4b59-a950-ad45c50ebb97"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1126/1126 [==============================] - 7s 6ms/step - loss: 79317680.0000 - accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1126/1126 [==============================] - 6s 6ms/step - loss: 66467756.0000 - accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1126/1126 [==============================] - 6s 6ms/step - loss: 55413940.0000 - accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1126/1126 [==============================] - 6s 6ms/step - loss: 46369976.0000 - accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1126/1126 [==============================] - 6s 5ms/step - loss: 38657392.0000 - accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            " 948/1126 [========================>.....] - ETA: 0s - loss: 31484426.0000 - accuracy: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-05fef6b52da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#history = ann.fit(train_ds, validation_data=val_ds, epochs=epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model\n",
        "loss, accuracy = ann.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# show the result\n",
        "history_dict = history.history\n",
        "#history_dict.keys()\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# show loss\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracy\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "DVnH2RYXeViB",
        "outputId": "a9d9a5a2-1771-4baa-a772-32a759fd4579"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 [==============================] - 0s 2ms/step - loss: -29927.7344 - binary_accuracy: 0.1177\n",
            "Loss:  -29927.734375\n",
            "Accuracy:  0.11772545427083969\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-61a865ea3714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_binary_accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "CSV reader example code:\\\n",
        "https://realpython.com/python-csv/\n",
        "\n",
        "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "\n",
        "https://www.geeksforgeeks.org/python-stemming-words-with-nltk/?ref=lbp\n",
        "\n",
        "https://stackoverflow.com/questions/29314033/drop-rows-containing-empty-cells-from-a-pandas-dataframe\n",
        "\n",
        "https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
        "\n",
        "https://towardsdatascience.com/heres-the-most-efficient-way-to-iterate-through-your-pandas-dataframe-4dad88ac92ee"
      ],
      "metadata": {
        "id": "mt01xYnnkhzr"
      }
    }
  ]
}